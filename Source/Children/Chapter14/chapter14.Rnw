% Chapter Chapter 14 For Reproducible Research in R and RStudio
% Christopher Gandrud
% Created: 16/07/2012 05:45:03 pm CEST
% Updated: 5 May 2015

<<set-parent14, echo=FALSE, results='hide', cache=FALSE>>=
set_parent('Rep-Res-Parent.Rnw')
@

\chapter{Conclusion}\label{FinalChapter}

\begin{quote}
    \emph{Well, we have completed our journey. The only thing left to do now is practice, practice, practice.} \citep[][432]{ShottsJr2012}
\end{quote}

In this book we learned a workflow for highly reproducible computational research and many of the tools needed to actually do it. Hopefully, if you haven't already, you will begin using and benefiting from these tools in your own work. Though we've covered enough material in this book to get you well on your way, there is still a lot more to learn. With most things computational (possibly most things in general), one of the best ways to continue learning is to practice and try new things. Inevitably you will hit walls, but there are almost always solutions that can be found with curiosity and patience. The R and reproducible research community is extremely helpful when it comes to finding and sharing solutions. I highly recommend getting involved in and eventually contributing to this community to get the most out of reproducible research.\footnote{A good point of entry into the R reproducible research community is R-bloggers (\url{http://www.r-bloggers.com/}). The site aggregates many different blogs on R-related topics from both advanced and relatively new R users. I have found that beyond just consuming other peoples' insights, contributing to R-bloggers--having to clearly write down my steps--has sharpened my understanding of the reproducible research process and enabled me to get great feedback. Other really useful resources are the R Stack Overflow (\url{http://stackoverflow.com/questions/tagged/r}) and Cross Validated (\url{http://stats.stackexchange.com/questions/tagged/r}) sites.}

Before ending the book, I want to briefly address five issues we have not covered so far that are important for reproducible research: citing reproducible research, licensing this research, sharing your code with R packages, whether or not to make your research files public before publishing the results, and whether or not it is possible to completely future-proof your research.

\section{Citing Reproducible Research}\index{citing}

There are a number of well-established methods for citing presentation documents, especially published articles and books. However, as we discussed in the beginning, these documents are just the advertising for research findings rather than the actual research \cite[385]{Buckheit1995,Donoho2010}. If other researchers are going to use the data and source code used to create the findings in their own work, they need a way of actually citing the particular data and source code they used. Citing data and source code presents unique problems. Data and source code can change and be updated over time in a way that published articles and books generally are not. As such we have a much less developed, or at least less commonly used set of standards for citing these types of materials.

One possibility is a standard for citing quantitative data sets laid out by \cite{Altman2007} \citep[see also][]{King2007}. They argue that quantitative data set citations should:

\begin{itemize}
    \item allow a reader to quickly understand the nature of the cited data set,
    \item unambiguously identify a particular version of the data set, and
    \item enable reliable location, retrieval, and verification of the data set.
\end{itemize}

\noindent The first issue can be solved by having a citation that includes the author, the date the data set was made public, and its title. However, these things do not unambiguously identify the data set as it may be updated or changed and it does not enable its location and retrieval. To solve this problem, \citeauthor{Altman2007} suggest that these citations also include:

\begin{itemize}
    \item a unique global identifier (UGI),\index{UGI}
    \item a universal numeric fingerprint (UNF),\index{UNF} and
    \item a bridge service.\index{bridge service}
\end{itemize}

\noindent A UGI uniquely identifies the data set. Examples include Document Object Identifiers (DOI)\index{DOI} and the Handel System.\footnote{See: \url{http://www.handle.net/}.}\index{Handel System} UGIs by themselves do not uniquely identify a particular version of a data set. This is where UNFs come in. They uniquely identify each version of a data set. Finally, a bridge service links the UGI and UNF to an actual document, usually posted online, so that it can be retrieved.

There are many ways to register DOIs and Handel UGIs. Most of these also include means for creating UNFs and a bridge service.\index{bridge service} Examples of services that store your work and assign it DOIs are figshare\index{figshare}\footnote{\url{http://figshare.com/}} and Zenodo.\index{Zenodo}\footnote{\url{https://zenodo.org/}} Zenodo can be integrated with GitHub so that it will store and create citations for a specific commit of a GitHub repository whenever you create a tag.\index{git command!tag} For more information about integrating GitHub and Zenodo see: \url{https://guides.GitHub.com/activities/citable-code/}. Please see \cite{Altman2007} for details of other services.\footnote{The Dataverse Project (\url{http://thedata.org/}) offers a free service to host files that also uses the Handel System to assign UGIs, UNFs, and provides a bridge service. See \cite{Gandrud2013} for a comparison of Dataverse\index{Dataverse Project} with GitHub\index{GitHub} and Dropbox\index{Dropbox} for data storage.}

Though \citeauthor{Altman2007} are interested in data sets, their system could easily be applied to source code as well. UGIs could identify a source code file or collection of files. The UNF could identify a particular version and a bridge service would create a link to the actual files.

\section{Licensing Your Reproducible Research}\index{licensing}

In the United States and many other countries, research, including computer code made available via the internet, is automatically given copyright protection. However, copyright protection works against the scientific goals of reproducible research, because work derived from the research falls under the original copyright protections \cite[36]{Stodden2009}. To solve this problem, some authors have suggested placing code under an open source software license like the GNU General Public License (GPL) \cite[]{Vandewalle2007}.\index{GNU General Public License} \cite{Stodden2009} argues that this type of license is not really adequate for making available the data, code, and other material needed to reproduce research findings in a way that enables scientific validation and knowledge growth. I don't want to explore the intricacies of these issues here. Nonetheless, they are important for computational researchers to think about, especially if their data and source code is publicly available. Two good places to go for more information are \cite{Stodden2009} and \cite{CreativeCommons2012}.

\section{Sharing Your Code in Packages}\index{R!package development}\index{R!functions}

Developing R functions and putting them into packages is a good way to enable cumulative knowledge development. Many researchers spend a considerable amount of time writing code to solve problems that no one has addressed yet, or haven't addressed in a way that they believe is adequate. It is very useful if they make this code publicly accessible so that others can perhaps adopt and use it in their own work without having to duplicate the effort used to create the original functions. Abstracting your code into functions so that they can be applied to many problems and distributing them in easily installed packages makes it much easier for other researchers to adopt and use your code to help solve their research problems. The active community of researcher/package developers is one of the main reasons that R has become such a widely used and useful statistical language.

Many of the tools we have covered in this book provide a good basis to start making and distributing functions. We have discussed many of the R commands and concepts that are important for creating functions. We have also looked at Git and GitHub, which are very helpful for developing and distributing packages. Learning about Hadley Wickham's \emph{devtools} package is probably the best next step for you to take to be able to develop and distribute functions in packages. He has an excellent introduction to \emph{devtools} and R package development in general at \url{http://adv-r.had.co.nz/Philosophy.html#introduction-to-devtools}.

RStudio Projects\index{RStudio!Projects}\index{RStudio!package development} have excellent \emph{devtools} integration and are certainly worth using. To begin creating a new package in RStudio, start a new project, preferably with Git version control (see Section \ref{NewProjectGit}). In the \textbf{New Project} window select \texttt{Package}. Now you will have a new Project with all of the files and directories you need to get started making packages that will hopefully be directly useful for the computational research community.

\section{Project Development: Public or Private?}

Hopefully I have made a convincing case in this book that research results, especially in academia, should almost always be highly reproducible. The files used to create the results need to be publicly available for the research to be really reproducible.\footnote{There are obvious exceptions, such as when a study's participants' identities need to remain confidential.} During the development of a research project, however, should files be public or private?

On the one hand, openness encourages transparency and feedback. Other researchers may alert you to mistakes before a result is published. On the other hand, there are worries that you may be ``scooped''. Another researcher might see your files, take your idea, and publish it before you have a chance to. In general, this worry may be a bit overblown. Especially if you use a version control\index{version control} system that clearly dates all of your file versions, it would be very easy to make the case that someone has stolen your work. Hopefully this possibility would discourage any malfeasance. That being said, unlike the clear need to make research files available after publication, during research development there are good reasons for both making files public and keeping them private.

Researchers should probably make this decision on a case-by-case basis. In general, I choose to make my research repositories public to increase transparency and encourage feedback. The community of researchers in my field is relatively small and close knit. It would be hard for someone to take my work and pass it off as their own. This is especially true if many people already know that they are my ideas, because I have made by research files publicly available.  However, during the development of this book, which has a more general appeal, I kept the repository private to avoid being ``scooped''. Regardless, cloud storage systems like GitHub\index{GitHub} make it easy to choose whether or not to make your files public or private. You can easily keep a repository private while you create a piece of research and then make it public once the results are published.

\section{Is it Possible to Completely Future-Proof Your Research?}

\index{future-proof research|(}

In this book we've looked at a number of ways to help future-proof\index{future-proof} your research so that future researchers (and you) are able to actually reproduce it. These included storing your research in text files, clearly commenting on your code, and recording information about the software environment you used by, for example, recording your session info.\index{session info} Are these steps enough to completely ensure that your research will always be reproducible? The simple answer is probably no. Software changes, but it is difficult to foresee what these changes will be. Nonetheless, beyond what we have discussed so far there are other steps we can take to make our reproducible research as future-proof as possible.

One of the main obstacles to completely future-proofing your research is that no (or at least very few) pieces of software are complete. R packages are updated. R is updated. Your operating system is updated. These and other software programs discussed in this book may not only be updated, but also discontinued. Changes to the software you used to find your results may change the results someone reproducing your research gets. This problem becomes larger as you use more pieces of software in your research

That being said, many of the software tools we have learned about in this book have future-proofing at their heart. TeX, the typesetting system that underlies LaTeX, is probably the best example. TeX was created in 1978 and has since been maintained with future-proofing in mind \citep{Knuth1990}. Though changes and new versions continue to be made, we are still able to use TeX to recreate documents in their original intended form even if they were written over thirty years ago. We also saw that, though R and especially R packages change rapidly, the Comprehensive R Archive Network\index{CRAN} stores and makes accessible old versions (as the name suggests). Old versions can be downloaded by anyone wishing to reproduce a piece of research, provided the original researcher has recorded which versions they used. This is very easy using \emph{repmis}'s \texttt{LoadandCite} command.\index{R function!LoadandCite} This command lets you specify particular package versions to install and load from the CRAN package archive.\footnote{Do this by entering specific package version numbers in the \texttt{versions} argument.} Another approach is to use the \emph{packrat} \index{packrat}\index{dependency management} R package \citep{R-packrat} for managing the packages your project depends on. Some of the other technologies discussed in this book may be less reliable over time, so some caution should be taken if you intend to use them to create fully reproducible research.

In addition to documenting what software you used and using software that archives old versions, some have suggested another step to future-proof reproducible research: encapsulate it in a virtual machine\index{virtual machine} that is available on a cloud storage system. See in particular \cite{Howe2012}. A virtual reproducible research machine would store a ``snapshot [of] a researcher's entire working environment, including data, software, dependencies, notes, logs, scripts, and more''. If the virtual machine is stored on a cloud server, then anyone wanting to reproduce the research could access the full computing environment used to create a piece of research \citep[36]{Howe2012}. As long as others could run the virtual machine and access the cloud storage system, you would not have to worry about changing software, because the exact versions of the software you used would be available in one place.

We don't have space to cover the specifics of how to create a virtual machine in this book. However, using a virtual machine is a tool that can be added to the workflow discussed in this book, rather than being a replacement for it. Carefully documenting your steps, clearly organizing your files, and dynamically tying together your data gathering, analysis, and presentation files helps you and others understand how you created a result after a research project's results have been published. Being able to understand your research will give it higher research impact as others can more easily build on it. The steps covered in this book will still encourage you to have better work habits from the beginning of your research projects even if you will be using a virtual machine. The tools and workflow will also continue to facilitate collaboration and make it easier to dynamically update your research documents when you make changes.

\index{future-proof research|)}

\vspace{1cm}

\noindent Now, get started with reproducible research!
